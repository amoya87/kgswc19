Given the scores as they stand, it looks like the paper is leaning towards a reject. While we disagree with the verdict, we are admittedly somewhat biased, and the reviewers' comments are very constructive. We hence take this opportunity to thank the reviewers for their time and thoughts, and to express some high level perspectives regarding the work and the paper.

# Reviewer 1:

The reviewer notes that the paper lacks technical depth, and that applying machine learning techniques seems straightforward. We accept this point of view but we saw/see the contribution as going beyond the solution proposed. We submitted the paper as we felt that the novelty was in the problem itself: establishing the conceptual framework for it, putting together the benchmark, identifying the trade-offs, giving initial results. We felt (perhaps optimistically) that what we had done required more thought and (conceptual/physical) effort than what would be needed to publish another algorithm for an incremental improvement on an existing problem with bibliographies, benchmarks, datasets, problem statements, etc., already available. It might even be considered in its own way as more novel: if we define novelty exclusively in terms of novel *algorithms*, we perhaps run the risk of disincentivising the study of new problems, rather incentivising incremental work that beats the state-of-the-art.

Maybe ISWC could use a track for work on new problems, or maybe the work might just be better suited to a workshop (and then we can work on new algorithms for a conference submission).


# Reviewer 2:

We very much appreciate the detailed comments. There are plenty of great suggestions for us to explore (though perhaps taken together they go beyond the scope of a conference paper?). Regarding the novel features related to the query, we were confident these would be more useful than they were. Intuitively we figured that a query with more triple patterns and more variables had higher probability to "hit" something that changed; of course static query features would not suffice to beat features based on historical results, but they would be a lot cheaper to manage/compute than historical features. Unfortunately the query features did not turn out to be very useful. Correlation analysis surprisingly showed no relation between the size of the query (number of variables, number of triple patterns, etc.) and how dynamic its results were. We still don't know why this is, but possibly it's because adding more triple patterns might actually help focus the query (e.g., { (?s a :Person) } will be more dynamic than { (?s a :Person), (?s :name "Bill Gates") }); in any case, our initial hypothesis was mostly rejected. As the deadline approached, the paper was a bit stuck between what we wrote before the results, and what we wrote after them. A more appropriate narrative would have been a negative result (but these can be hard to sell).

On a side note, we of course plan to make it up to the authors of [11] for using their data without full credit (though really we just used their dumps; most work was in the queries, deltas, feature extraction, etc.).

# Reviewer 3:

We appreciate that the reviewer recognises the (months of) work that went into the benchmark; we were very happy after all that work when we saw that the queries happened to be almost perfectly balanced for the static/dynamic classes. The reviewer is correct that the model is probably not very generalisable, at least with predicate features; indeed the model will have to be trained for a specific dataset (though maybe transfer learning could be applied; or alternatively a more generalisable model might be just to use the 1/0 feature vector of past changes). Regarding the cost of annotation, we disagree (or maybe misunderstand) the part that states "your approach will be as expensive": static query features and predicate features are much cheaper to work with. (Maybe we misunderstood.) We accept the point about the straightforward approach (as mentioned by reviewer 1).

# Reviewer 4:

We had not considered the resource track; maybe in retrospect that might have been a better option. We acknowledge that more queries and datasets would be better, though the ones we have already required a lot of work and we felt they would be sufficient to establish initial conclusions for the problem (then again, maybe our query features will work better for other datasets). As before, we accept the reviewer's opinion that the technical contribution is not that high; actually we agree a lot with (W2). We promise that we had planned on sharing the code after double-blind review and accept the point about supplementary material.